<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ObjectVLA: End-to-End Open-World Object Manipulation with Vision-Language-Action Model">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ObjectVLA: End-to-End Open-World Object Manipulation with Vision-Language-Action Model</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    .video-slider-container {
      display: flex;
      align-items: center;
      justify-content: center;
      position: relative;
    }
    .video-slider {
      width: 100%;
      max-width: 400px;
      display: flex;
      justify-content: center;
    }
    .slider-btn {
      background-color: rgba(0, 0, 0, 0.5);
      color: white;
      border: none;
      padding: 10px;
      cursor: pointer;
      border-radius: 5px;
    }
    .slider-btn:hover {
      background-color: rgba(0, 0, 0, 0.8);
    }
  </style>
  <script>
document.addEventListener("DOMContentLoaded", function () {
    let currentIndex = 0;
    const videos = document.querySelectorAll(".video-slide");

    function showVideo(index) {
        videos.forEach((video, i) => {
            video.style.display = i === index ? "block" : "none";
        });
    }

    function prevVideo() {
        currentIndex = (currentIndex === 0) ? videos.length - 1 : currentIndex - 1;
        showVideo(currentIndex);
    }

    function nextVideo() {
        currentIndex = (currentIndex === videos.length - 1) ? 0 : currentIndex + 1;
        showVideo(currentIndex);
    }

    document.querySelector(".slider-btn.left").addEventListener("click", prevVideo);
    document.querySelector(".slider-btn.right").addEventListener("click", nextVideo);
});

  </script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ObjectVLA: End-to-End Open-World Object<br>Manipulation with Vision-Language-Action Model</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a>Minjie Zhu</a><sup>1,2,*</sup>,</span>
            <span class="author-block">
              <a>Yichen Zhu</a><sup>1,*,&dagger;</sup>,</span>
            <span class="author-block">
              <a>Jinming Li</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a>Zhibing Tang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a>Zhongyi Zhou</a><sup>1,2</sup>,
            </span>
            <br>
            <span class="author-block">
              <a>Junjie Wen</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a>Xiaoyu Liu</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a>Chaomin Shen</a><sup>2</sup>,</span>
            <span class="author-block">
              <a>Yaxin Peng</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a>Feifei Feng</a><sup>1</sup>,
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Midea Group,</span>
            <span class="author-block"><sup>2</sup>East China Normal University,</span>
            <span class="author-block"><sup>3</sup>Shanghai University</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal contribution</span>
            <span class="author-block"><sup>&dagger;</sup>Corresponding author</span>
            
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://objectvla.github.io/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://objectvla.github.io/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns">
      <div class="column has-text-centered">
        <img src="./static/images/objectvla.jpg"
              class="move-exp" style="max-width: 80%; height: auto;"/>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          Imitation learning has proven to be highly effective in teaching robots dexterous manipulation skills. 
          However, it typically relies on large amounts of human demonstration data, which limits its scalability and 
          applicability in dynamic, real-world environments. One key challenge in this context is object generalization—
          where a robot trained to perform a task with one object, such as ''hand over the apple,'' struggles to transfer 
          its skills to a semantically similar but visually different object, such as ''hand over the peach.'' 
          This gap in generalization to new objects beyond those in the same category has yet to be adequately addressed in 
          previous work on end-to-end visuomotor policy learning. In this paper, we present a simple yet effective approach for 
          achieving object generalization through Vision-Language-Action (VLA) models, referred to as <b style="font-weight:900;">ObjectVLA</b>. 
          Our model enables robots to generalize learned skills to novel objects without requiring explicit human demonstrations 
          for each new target object. By leveraging vision-language pair data, our method provides a lightweight and 
          scalable way to inject knowledge about the target object, establishing an implicit link between the object and the desired action. 
          We evaluate ObjectVLA on a real robotic platform, demonstrating its ability to generalize across over 100 novel objects with 
          a 64% success rate in selecting objects not seen during training. Furthermore, we propose a more accessible method for 
          enhancing object generalization in VLA models—using a smartphone to capture a few images and fine-tune the pre-trained model. 
          These results highlight the effectiveness of our approach in enabling object-level generalization and reducing the need for 
          extensive human demonstrations, paving the way for more flexible and scalable robotic learning systems.
        </div>
      </div>
    </div>

    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <!-- Experimental Results. -->
        <h2 class="title is-3 is-centered has-text-centered">Experiments</h2>
        <div class="content has-text-justified">
          <p>
            We start by evaluating the generalization performance of our ObjectVLA in selecting object. The experimental result is shown in the following figure.
            Our method achieves a 100% success rate for in-distribution objects selecting and a 64% success rate for out-of-distribution objects selecting.
            Notably, ObjectVLA w/o bbox achieves only a 19% success rate in OOD evaluation, despite achieving a 100% success rate in the ID test.
            This illustrates that without explicit grounding and a structured reasoning process, the model struggles to differentiate objects in vision-language data.
          </p>
          <div class="columns">
            <!-- 左侧图片 -->
            <div class="column has-text-centered">
              <img src="./static/images/move-exp.jpg" class="move-exp" style="max-width: 100%; height: auto;"/>
            </div>
            
            <!-- 右侧滑动组件 -->
            <div class="column has-text-centered">
              <div class="content">
                <p>
                    Rollouts
                </p>
              <div class="video-slider-container">
                <button class="slider-btn left" onclick="prevVideo()">&#9665;</button>
                <div class="video-slider">
                  <video class="video-slide" autoplay controls muted loop style="border-radius: 5px; display: block;">
                    <source src="static/videos/move_to/move-to-ood.mp4" type="video/mp4">
                  </video>
                  <video class="video-slide" autoplay controls muted loop style="border-radius: 5px; display: none;">
                    <source src="static/videos/move_to/move-to-id.mp4" type="video/mp4">
                  </video>
                </div>
                <button class="slider-btn right" onclick="nextVideo()">&#9655;</button>
              </div>
            </div>
          </div>
          
          <!-- <div class="columns">
            <div class="column has-text-centered">
              <img src="./static/images/move-exp.jpg"
                    class="move-exp" style="max-width: 100%; height: auto;"/>
            </div>

          


            <div class="column has-text-centered">
                <video poster="" id=""autoplay controls muted loop height="100%" playbackRate=2.0 style="border-radius: 5px;">
                    <source src="static/videos/move_to/move-to-ood.mp4" type="video/mp4">
                </video>
            </div>
            <div class="column has-text-centered">
                <video poster="" id=""autoplay controls muted loop height="100%" playbackRate=2.0 style="border-radius: 5px;">
                  <source src="static/videos/move_to/move-to-id.mp4" type="video/mp4">
              </video>
            </div>
          </div> -->
        </div>
        <br/>
        <!-- Qualitative Comparison. -->
        <h2 class="title is-4 is-centered has-text-centered">Combining with More Skills.</h2>
        <div class="content has-text-justified">
          <p>
            We also expands the evaluation to encompass more complex skills, specifically "pick & place", "push" and "rotate".
            Our experimental results show that ObjectVLA can transfer skills to objects unseen in robot data but present in vision-language data.
            The following video is played at 2× speed for better visualization.
          </p>
        </div>
        <div class="columns">
          <div class="column has-text-centered">
            <p style="font-size: 100%"><b>Bin Picking</b>
              <video poster="" id=""autoplay controls muted loop height="100%" playbackRate=2.0 style="border-radius: 5px;">
                  <source src="static/videos/bin-picking/bin-picking-id.mp4" type="video/mp4">
              </video>
          </div>
          <div class="column has-text-centered">
            <p style="font-size: 100%"><b>Rotate</b>
              <video poster="" id=""autoplay controls muted loop height="100%" playbackRate=2.0 style="border-radius: 5px;">
                  <source src="static/videos/rotate/rotate-id.mp4" type="video/mp4">
              </video>
          </div>
          <div class="column has-text-centered">
            <p style="font-size: 100%"><b>Push</b>
              <video poster="" id=""autoplay controls muted loop height="100%" playbackRate=2.0 style="border-radius: 5px;">
                  <source src="static/videos/push/push-id.mp4" type="video/mp4">
              </video>
          </div>
        </div>
        <div class="columns">
          <div class="column has-text-centered">
              <video poster="" id=""autoplay controls muted loop height="100%" playbackRate=2.0 style="border-radius: 5px;">
                  <source src="static/videos/bin-picking/bin-picking-ood.mp4" type="video/mp4">
              </video>
          </div>
          <div class="column has-text-centered">
              <video poster="" id=""autoplay controls muted loop height="100%" playbackRate=2.0 style="border-radius: 5px;">
                  <source src="static/videos/rotate/rotate-ood.mp4" type="video/mp4">
              </video>
          </div>
          <div class="column has-text-centered">
              <video poster="" id=""autoplay controls muted loop height="100%" playbackRate=2.0 style="border-radius: 5px;">
                  <source src="static/videos/push/push-ood.mp4" type="video/mp4">
              </video>
          </div>
        </div>
        <h2 class="title is-4 is-centered has-text-centered">Cheap Object Generalization via Smart-Phone Pictures</h2>
        <div class="content has-text-justified">
          <p>
            Additionally, we find that our ObjectVLA can generalize to novel objects with only a few images captured by a smartphone.   
            The video is 2x speed up for better visualization. 
          </p>
          <div class="columns">
            <div class="column has-text-centered">
              <p style="font-size: 100%"><b>Pikachu</b>
                <video poster="" id=""autoplay controls muted loop height="100%" playbackRate=2.0 style="border-radius: 5px;">
                    <source src="static/videos/smart-phone/smart-phone-pikachu.mp4" type="video/mp4">
                </video>
            </div>
            <div class="column has-text-centered">
              <p style="font-size: 100%"><b>Toy Cat</b>
                <video poster="" id=""autoplay controls muted loop height="100%" playbackRate=2.0 style="border-radius: 5px;">
                    <source src="static/videos/smart-phone/smart-phone-toy-cat.mp4" type="video/mp4">
                </video>
            </div>
        </div>
        <!--/ Experimental Results. -->
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  title     = {ObjectVLA: End-to-End Open-World Object Manipulation with Vision-Language-Action Model},
  author    = {Zhu, Minjie and Zhu, Yichen and Li Jinming},
  booktitle = {Arxiv},
  year      = {2025},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a
                                href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
                                and <a href="https://eureka-research.github.io/">Eureka</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
